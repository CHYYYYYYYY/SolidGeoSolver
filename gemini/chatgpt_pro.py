#!/usr/bin/env python3
"""
ChatGPT CDL Generation Script
åŸºäºGeminiå®ç°æ”¹é€ ,ä½¿ç”¨OpenAI APIç”Ÿæˆå‡ ä½•é—®é¢˜çš„CDLè¡¨ç¤º
"""

from openai import OpenAI
import PIL.Image
import os
import json
import time
import base64
import socket
from io import BytesIO
from tqdm import tqdm
from pydantic import BaseModel, Field
from typing import List, Dict, Any

# --- é…ç½® ---
# ä½¿ç”¨OpenAIå…¼å®¹çš„API
OPENAI_API_KEY = "sk-oMHOiiySXVtKYBHBK2QJNlVWpwNC228JHTJrl824UdcV735S"
OPENAI_API_BASE = "https://aicanapi.com/v1"  # ChatGPT API base URL

if not OPENAI_API_KEY:
    raise ValueError("APIå¯†é’¥ä¸èƒ½ä¸ºç©º")

# åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
client = OpenAI(
    api_key=OPENAI_API_KEY,
    base_url=OPENAI_API_BASE
)

# --- 1. ä½¿ç”¨Pydanticå®šä¹‰ä¸¥æ ¼çš„JSONè¾“å‡ºç»“æ„ (Schema) ---
# å®Œæ•´Schemaï¼ˆç”¨äºæœ€ç»ˆè¾“å‡ºï¼‰
class ProblemSchema(BaseModel):
    problem_id: int = Field(default=0, description="é—®é¢˜çš„å”¯ä¸€æ ‡è¯†ç¬¦")
    annotation: str = Field(default="", description="æ ‡æ³¨ä¿¡æ¯")
    source: str = Field(default="SolidGeo", description="é—®é¢˜çš„æ¥æº")
    problem_text_en: str = Field(default="", description="é—®é¢˜çš„å®Œæ•´è‹±æ–‡è‡ªç„¶è¯­è¨€æè¿°")
    construction_cdl: List[str] = Field(default_factory=list, description="å®šä¹‰å‡ ä½•å®ä½“æ„é€ çš„è¾…åŠ©è°“è¯ï¼ŒåŒ…æ‹¬Shapeã€Collinearã€Cocircularã€Coplanarã€Cosphericalç­‰ã€‚ç”¨äºå®šä¹‰å½¢çŠ¶çš„è¾¹ã€çº¿æ®µå’Œç‚¹çš„å‡ ä½•å…³ç³»ã€‚")
    text_cdl: List[str] = Field(default_factory=list, description="ä»…ä»ã€æ–‡æœ¬æè¿°ã€‘ä¸­æå–çš„å‡ ä½•å…³ç³»å’Œæ¡ä»¶")
    image_cdl: List[str] = Field(default_factory=list, description="ä»…ä»ã€å›¾ç‰‡ã€‘ä¸­æå–çš„å‡ ä½•å…³ç³»å’Œæ¡ä»¶")
    goal_cdl: str = Field(default="", description="æ±‚è§£ç›®æ ‡ï¼Œå¿…é¡»ä»¥ 'Value(...)' çš„å½¢å¼è¡¨ç¤º")
    problem_answer: str = Field(default="", description="é—®é¢˜çš„æ ‡å‡†ç­”æ¡ˆ")
    problem_type: List[str] = Field(default_factory=list, description="é—®é¢˜çš„ç±»å‹åˆ†ç±»")
    complexity_level: str = Field(default="", description="é—®é¢˜çš„å¤æ‚åº¦çº§åˆ«")
    theorem_seqs: List[str] = Field(default_factory=list, description="è§£å†³é—®é¢˜æ‰€éœ€çš„å®šç†åºåˆ—")
    theorem_seqs_dag: str = Field(default="{\"START\": []}", description="å®šç†çš„æœ‰å‘æ— ç¯å›¾")

# APIè¿”å›çš„ç®€åŒ–Schemaï¼ˆåªåŒ…å«æ ¸å¿ƒCDLå­—æ®µï¼‰
class CDLSchema(BaseModel):
    construction_cdl: List[str] = Field(default_factory=list)
    text_cdl: List[str] = Field(default_factory=list)
    image_cdl: List[str] = Field(default_factory=list)
    goal_cdl: str = Field(default="")
    problem_answer: str = Field(default="")
# --- 2. ä¼˜åŒ–åçš„é»„é‡‘æç¤ºè¯æ¨¡æ¿ï¼ˆå‚è€ƒGeminiä»£ç é£æ ¼ï¼‰ ---
PROMPT_TEMPLATE = """You are an expert in generating CDL for geometry, and must strictly follow the format and rules of the following training examples to generate CDL:

**Rule 0: [Predicate Compliance - MOST IMPORTANT!]**
All CDL predicates you generate (e.g., `Equal`, `Cone`, `LengthOfLine`) **MUST** be strictly selected from the official list provided below.
You are absolutely forbidden from using any predicates not appearing in this list. This is a hard constraint.

--- Official Predicate List ---
{valid_predicates_str}
--- End of Official Predicate List ---

**Core Rules:**

1. **text_cdl**: Extract key geometric information from the problem text (if there is no text, fill in ["no relevant text information"]), and ensure that there are no spaces before or after the comma in any Equal(x,3)

2. **image_cdl**: Extract visual elements from the image (if there is no image, fill in ["no relevant image information"])

3. **construction_cdl**: Generate geometric construction steps (points are represented by single letters A/B/C/D, etc.)
   - **Shape Predicates**: Define edges or line segments of shapes
     * For line segments/edges: `Shape(AB,BC,CD,DA)` or `Shape(OP,PO)` or `Shape(PQ,QP)`
     * For points (spheres, etc.): `Shape(O)` or `Shape(P)`
   - **Collinear/Cocircular/Coplanar/Cospherical Predicates**: Define geometric relationships of points
     * `Collinear(PABQ)` - Points P, A, B, Q are collinear
     * `Cocircular(O)` - Point O is on a circle (used for cone/cylinder base center)
     * `Cocircular(P)`, `Cocircular(Q)` - Points P, Q are on their respective circles
     * `Coplanar(U,ABCD)` - Point U is coplanar with ABCD
     * `Cospherical(O)` - Point O is on a sphere

4. **goal_cdl**: Define the problem objective (when there is no text or image, make reasonable inferences based on the geometric scenario, such as "complete relevant calculations based on geometric shapes")

5. **problem_answer**: Must be a pure number or expression (e.g., "10", "254.47", "36*pi"), absolutely cannot contain units (like 'cm') or text (like 'Surface Area =').

### Training Examples (Total: {total_examples})
{training_examples}

### Test Task
Now please process the following test sample and strictly follow the above example format to generate a JSON-formatted CDL, with fields including:
- construction_cdl (List[str])
- text_cdl (List[str])
- image_cdl (List[str])
- goal_cdl (str)
- problem_answer (str)

Output only JSON, do not include other redundant content.
"""

def load_few_shot_examples(train_dir, images_dir, max_examples=5):
    """
    ä»manual_train_setç›®å½•åŠ è½½few-shotèŒƒä¾‹ï¼Œå¹¶åŠ è½½å¯¹åº”çš„å›¾ç‰‡
    
    Args:
        train_dir: è®­ç»ƒèŒƒä¾‹ç›®å½•
        images_dir: å›¾ç‰‡ç›®å½•
        max_examples: æœ€å¤§èŒƒä¾‹æ•°é‡ï¼ˆé™åˆ¶æ•°é‡ä»¥é¿å…tokenè¿‡å¤šï¼‰
    
    Returns:
        examples_data: List[Dict] åŒ…å«èŒƒä¾‹æ•°æ®çš„åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«æ–‡æœ¬ã€å›¾ç‰‡base64å’ŒCDL
        valid_count: int æœ‰æ•ˆèŒƒä¾‹æ•°é‡
    """
    examples_data = []
    valid_count = 0
    
    try:
        # è·å–æ‰€æœ‰JSONæ–‡ä»¶å¹¶æ’åº
        train_json_files = [
            f for f in os.listdir(train_dir)
            if f.endswith('.json') and os.path.isfile(os.path.join(train_dir, f))
        ]
        train_json_files.sort(key=lambda x: int(x.split('.')[0]) if x.split('.')[0].isdigit() else 999999)
        
        for json_file in train_json_files:
            if valid_count >= max_examples:
                break
                
            problem_id = json_file.split('.')[0]
            json_path = os.path.join(train_dir, json_file)
            
            try:
                with open(json_path, 'r', encoding='utf-8') as f:
                    train_data = json.load(f)
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„CDLæ•°æ®
                has_valid_cdl = False
                cdl_fields = ['construction_cdl', 'text_cdl', 'image_cdl', 'goal_cdl']
                for field in cdl_fields:
                    val = train_data.get(field, [])
                    if (isinstance(val, list) and len(val) > 0) or (isinstance(val, str) and val.strip()):
                        has_valid_cdl = True
                        break
                
                if not has_valid_cdl:
                    continue
                
                # è·å–é—®é¢˜æ–‡æœ¬
                problem_text = train_data.get('problem_text_en', '').strip()
                if not problem_text:
                    # å°è¯•ä»txtæ–‡ä»¶è¯»å–
                    txt_path = os.path.join(train_dir, f"{problem_id}.txt")
                    if os.path.exists(txt_path):
                        with open(txt_path, 'r', encoding='utf-8') as f:
                            problem_text = f.read().strip()
                
                # è·å–å›¾ç‰‡å¹¶ç¼–ç ä¸ºbase64
                img_base64 = None
                image_extensions = ['.png', '.jpg', '.jpeg']
                for ext in image_extensions:
                    candidate_path = os.path.join(images_dir, f"{problem_id}{ext}")
                    if os.path.exists(candidate_path):
                        img_base64 = image_to_base64(candidate_path)
                        break
                
                # æ„å»ºèŒƒä¾‹æ•°æ®
                example_data = {
                    "problem_id": problem_id,
                    "problem_text": problem_text if problem_text else "æ— æ–‡æœ¬ä¿¡æ¯",
                    "image_base64": img_base64,
                    "text_cdl": train_data.get('text_cdl', []),
                    "image_cdl": train_data.get('image_cdl', []),
                    "construction_cdl": train_data.get('construction_cdl', []),
                    "goal_cdl": train_data.get('goal_cdl', ''),
                    "problem_answer": train_data.get('problem_answer', '')
                }
                
                examples_data.append(example_data)
                valid_count += 1
                
            except Exception as e:
                print(f"è·³è¿‡è®­ç»ƒæ ·æœ¬{problem_id}ï¼ˆåŠ è½½å¤±è´¥ï¼‰: {str(e)}")
                continue
        
        print(f"æˆåŠŸåŠ è½½ {valid_count} ä¸ªè®­ç»ƒèŒƒä¾‹ï¼ˆåŒ…å«å›¾ç‰‡ï¼‰")
        
    except Exception as e:
        print(f"åŠ è½½è®­ç»ƒèŒƒä¾‹æ—¶å‡ºé”™: {e}")
        return [], 0
        
    return examples_data, valid_count

def load_valid_predicates(gdl_path):
    """ä» predicate_GDL.json æ–‡ä»¶ä¸­åŠ è½½æ‰€æœ‰åˆæ³•çš„è°“è¯åç§°ã€‚"""
    try:
        with open(gdl_path, 'r', encoding='utf-8') as f:
            gdl_data = json.load(f)
        
        predicate_names = set()
        
        # æå– "Entity", "Relation", "Attribution" ä¸­çš„æ‰€æœ‰è°“è¯
        for category in ["Entity", "Relation", "Attribution"]:
            if category in gdl_data:
                for key in gdl_data[category].keys():
                    predicate_name = key.split('(')[0]
                    predicate_names.add(predicate_name)
                    
        # ä¹Ÿæ·»åŠ ä¸€äº›å¯èƒ½çš„åŸºç¡€å®ä½“è°“è¯
        for preset_key in ["BasicEntity", "Construction"]:
            if preset_key in gdl_data.get("Preset", {}):
                for name in gdl_data["Preset"][preset_key]:
                    predicate_names.add(name)
        
        # æ·»åŠ  Equal è°“è¯
        predicate_names.add("Equal")
        predicate_names.add("Value")

        return sorted(predicate_names)
    except Exception as e:
        print(f"Error loading predicates from '{gdl_path}': {e}")
        return None

def fix_incomplete_json(json_str):
    """Try to fix incomplete JSON strings"""
    try:
        # Try direct parsing
        return json.loads(json_str)
    except json.JSONDecodeError:
        pass
    
    # Try to fix common JSON truncation issues
    fixed_str = json_str.strip()
    
    # If starts with { but doesn't end with }, try to add it
    if fixed_str.startswith('{') and not fixed_str.rstrip().endswith('}'):
        # Find the last complete key-value pair
        last_comma = fixed_str.rfind(',')
        if last_comma > 0:
            # Remove the last incomplete key-value pair
            fixed_str = fixed_str[:last_comma] + '}'
        else:
            fixed_str = fixed_str + '}'
    
    # Try to parse the fixed string
    try:
        return json.loads(fixed_str)
    except json.JSONDecodeError:
        return None

def normalize_cdl_data(cdl_data):
    """Normalize CDL data to ensure it's a list of strings"""
    if cdl_data is None:
        return []
    if isinstance(cdl_data, list):
        result = []
        for item in cdl_data:
            if isinstance(item, str):
                result.append(item)
            elif isinstance(item, dict):
                # Try to extract predicate string from dict
                # e.g., {"predicate": "Cylinder", "params": ["O", "P"]} -> "Cylinder(O,P)"
                predicate = item.get('predicate') or (list(item.keys())[0] if item else None)
                if predicate:
                    params = item.get('params') or item.get(predicate) or []
                    if isinstance(params, list):
                        params_str = ','.join(str(p) for p in params)
                        result.append(f"{predicate}({params_str})")
                    else:
                        result.append(str(predicate))
        return result
    return []

def image_to_base64(image_path):
    """å°†å›¾ç‰‡è½¬æ¢ä¸ºbase64ç¼–ç """
    try:
        with open(image_path, 'rb') as image_file:
            encoded_string = base64.b64encode(image_file.read()).decode('utf-8')
            # æ£€æµ‹å›¾ç‰‡æ ¼å¼
            ext = os.path.splitext(image_path)[1].lower()
            if ext in ['.jpg', '.jpeg']:
                mime_type = 'image/jpeg'
            elif ext == '.png':
                mime_type = 'image/png'
            else:
                mime_type = 'image/jpeg'
            return f"data:{mime_type};base64,{encoded_string}"
    except Exception as e:
        print(f"Error encoding image {image_path}: {e}")
        return None

def generate_geometry_json(problem_text, image_path, golden_prompt, problem_id, few_shot_examples_data=None, retries=3, delay=5):
    """
    è°ƒç”¨ChatGPT APIç”Ÿæˆå•ä¸ªå‡ ä½•é—®é¢˜çš„JSONï¼Œä½¿ç”¨ç»“æ„åŒ–è¾“å‡ºã€‚
    
    Args:
        problem_text: é—®é¢˜æ–‡æœ¬ï¼ˆå¯ä»¥ä¸ºç©ºï¼‰
        image_path: é—®é¢˜å›¾ç‰‡è·¯å¾„ï¼ˆå¯ä»¥ä¸ºNoneï¼Œè¡¨ç¤ºæ²¡æœ‰å›¾ç‰‡ï¼‰
        golden_prompt: åŸºç¡€æç¤ºè¯
        problem_id: é—®é¢˜ID
        few_shot_examples_data: Few-shotèŒƒä¾‹æ•°æ®åˆ—è¡¨ï¼ˆåŒ…å«å›¾ç‰‡ï¼‰
        retries: é‡è¯•æ¬¡æ•°
        delay: é‡è¯•å»¶è¿Ÿ
    """
    # å‡†å¤‡å›¾ç‰‡ï¼ˆå¦‚æœæä¾›ï¼‰
    image_base64 = None
    if image_path:
        print(f"    ğŸ–¼ï¸  Processing image: {os.path.basename(image_path)}")
        image_base64 = image_to_base64(image_path)
        if not image_base64:
            print(f"    âš ï¸  Warning: Failed to load image {image_path}, will process with text only.")
        else:
            print("    âœ… Image processing completed")
    else:
        print("    â„¹ï¸  No image provided, will process with text only.")

    # æ„å»ºæ¶ˆæ¯ - åŒ…å«few-shotèŒƒä¾‹
    messages = []
    
    # System messageåŒ…å«åŸºç¡€æç¤ºè¯
    messages.append({
        "role": "system",
        "content": golden_prompt + "\n\nIMPORTANT: Your JSON output should ONLY contain these fields: construction_cdl, text_cdl, image_cdl, goal_cdl, problem_answer"
    })
    
    # æ·»åŠ few-shotèŒƒä¾‹ï¼ˆå¦‚æœæä¾›ï¼‰
    if few_shot_examples_data:
        for i, example in enumerate(few_shot_examples_data, 1):
            # èŒƒä¾‹çš„user message
            example_content = [
                {
                    "type": "text",
                    "text": f"Example {i} (ID: {example['problem_id']}):\nNatural Language Description: \"{example['problem_text']}\"\n\nPlease analyze the image and text, then generate the CDL fields in JSON format."
                }
            ]
            
            # å¦‚æœèŒƒä¾‹æœ‰å›¾ç‰‡ï¼Œæ·»åŠ åˆ°æ¶ˆæ¯ä¸­
            if example.get('image_base64'):
                example_content.append({
                    "type": "image_url",
                    "image_url": {
                        "url": example['image_base64']
                    }
                })
            
            messages.append({
                "role": "user",
                "content": example_content
            })
            
            # èŒƒä¾‹çš„assistantå›å¤ï¼ˆåŒ…å«CDLè¾“å‡ºï¼‰
            messages.append({
                "role": "assistant",
                "content": json.dumps({
                    "construction_cdl": example['construction_cdl'],
                    "text_cdl": example['text_cdl'],
                    "image_cdl": example['image_cdl'],
                    "goal_cdl": example['goal_cdl'],
                    "problem_answer": example['problem_answer']
                }, ensure_ascii=False)
            })
    
    # å½“å‰é—®é¢˜çš„user message
    user_content = [
        {
            "type": "text",
            "text": f"Now process this problem (ID: {problem_id}):\nNatural Language Description: \"{problem_text if problem_text else '(No text description, analyze the image only)'}\"\n\nPlease analyze the image and text, then generate ONLY the CDL fields in JSON format."
        }
    ]
    
    # å¦‚æœæœ‰å›¾ç‰‡ï¼Œæ·»åŠ åˆ°æ¶ˆæ¯ä¸­
    if image_base64:
        user_content.append({
            "type": "image_url",
            "image_url": {
                "url": image_base64
            }
        })
    
    messages.append({
        "role": "user",
        "content": user_content
    })

    for attempt in range(retries):
        try:
            if attempt > 0:
                print(f"    ğŸ”„ Retry attempt {attempt + 1}...")
            
            print(f"    â³ Sending API request to gpt-4o...")
            print(f"    â„¹ï¸  Request contains {len(messages)} messages with {len(few_shot_examples_data) if few_shot_examples_data else 0} few-shot examples")
            
            # è®¾ç½®socketè¶…æ—¶ï¼ˆ120ç§’ï¼‰
            old_timeout = socket.getdefaulttimeout()
            socket.setdefaulttimeout(120)
            
            try:
                response = client.chat.completions.create(
                    model="gpt-4o",  # ä½¿ç”¨æ”¯æŒè§†è§‰çš„æ¨¡å‹
                    messages=messages,
                    response_format={"type": "json_object"},
                    temperature=0.1,
                    max_tokens=4096,
                    timeout=120.0  # 120ç§’è¶…æ—¶
                )
                print(f"    ğŸ“¥ Received API response")
            except Exception as api_error:
                error_msg = f"API call failed: {str(api_error)}"
                print(f"    âŒ {error_msg}")
                # æ¢å¤åŸæ¥çš„è¶…æ—¶è®¾ç½®
                socket.setdefaulttimeout(old_timeout)
                if attempt == retries - 1:
                    return {"status": "error", "message": error_msg, "problem_text": problem_text}
                continue
            finally:
                # æ¢å¤åŸæ¥çš„è¶…æ—¶è®¾ç½®
                socket.setdefaulttimeout(old_timeout)
            
            # è§£æå“åº”
            if response.choices and response.choices[0].message.content:
                content = response.choices[0].message.content.strip()
                
                # Try to parse JSON (may be incomplete)
                try:
                    parsed_data = json.loads(content)
                except json.JSONDecodeError:
                    # Try to fix incomplete JSON
                    print(f"    ğŸ”§ Attempting to fix incomplete JSON...")
                    parsed_data = fix_incomplete_json(content)
                    if parsed_data is None:
                        error_message = f"JSON parsing failed and cannot be fixed (attempt {attempt + 1}/{retries})"
                        print(f"Error/Warning: {error_message}")
                        if attempt == retries - 1:
                            return {"status": "error", "message": error_message, "raw_output": content, "problem_text": problem_text}
                        continue
                
                # Normalize CDL data format
                print(f"    ğŸ”„ Normalizing data format...")
                parsed_data['construction_cdl'] = normalize_cdl_data(parsed_data.get('construction_cdl'))
                parsed_data['text_cdl'] = normalize_cdl_data(parsed_data.get('text_cdl'))
                parsed_data['image_cdl'] = normalize_cdl_data(parsed_data.get('image_cdl'))
                
                # Fix goal_cdl format: if it's a list, take the first element
                if isinstance(parsed_data.get('goal_cdl'), list):
                    if parsed_data['goal_cdl']:  # Non-empty list
                        parsed_data['goal_cdl'] = parsed_data['goal_cdl'][0]
                    else:  # Empty list
                        parsed_data['goal_cdl'] = ""
                elif not isinstance(parsed_data.get('goal_cdl'), str):
                    parsed_data['goal_cdl'] = str(parsed_data.get('goal_cdl', ''))
                
                # Use simplified CDLSchema for validation
                cdl_data = CDLSchema(**parsed_data)
                
                # è¡¥å…¨å®Œæ•´çš„ProblemSchemaå­—æ®µ
                full_data = ProblemSchema(
                    problem_id=problem_id,
                    annotation="",
                    source="SolidGeo",
                    problem_text_en=problem_text,
                    construction_cdl=cdl_data.construction_cdl,
                    text_cdl=cdl_data.text_cdl,
                    image_cdl=cdl_data.image_cdl,
                    goal_cdl=cdl_data.goal_cdl,
                    problem_answer=cdl_data.problem_answer,
                    problem_type=[],
                    complexity_level="",
                    theorem_seqs=[],
                    theorem_seqs_dag="{\"START\": []}"
                )
                
                return {"status": "success", "data": full_data.dict(), "problem_text": problem_text}
            else:
                raise ValueError("APIè¿”å›å†…å®¹ä¸ºç©ºã€‚")

        except Exception as e:
            error_message = f"APIè°ƒç”¨æˆ–è§£æå¤±è´¥ (å°è¯• {attempt + 1}/{retries}): {e}"
            print(f"é”™è¯¯/è­¦å‘Š: {error_message}")
            
            if attempt == retries - 1:
                return {"status": "error", "message": error_message, "problem_text": problem_text}
        
        time.sleep(delay)

    return {"status": "error", "message": "å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°", "problem_text": problem_text}


def batch_process_problems(input_dir, output_dir, example_dir, gdl_path, start_id=1, end_id=None, force_regenerate=False):
    """
    æ‰¹é‡å¤„ç†æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰é—®é¢˜ã€‚
    
    Args:
        input_dir: è¾“å…¥ç›®å½•
        output_dir: è¾“å‡ºç›®å½•
        example_dir: èŒƒä¾‹ç›®å½•
        gdl_path: è°“è¯å®šä¹‰æ–‡ä»¶è·¯å¾„
        start_id: èµ·å§‹é—®é¢˜IDï¼ˆåŒ…å«ï¼‰ï¼Œé»˜è®¤1
        end_id: ç»“æŸé—®é¢˜IDï¼ˆåŒ…å«ï¼‰ï¼Œé»˜è®¤Noneè¡¨ç¤ºä¸é™åˆ¶
        force_regenerate: æ˜¯å¦å¼ºåˆ¶é‡æ–°ç”Ÿæˆæ‰€æœ‰é—®é¢˜ï¼ˆå¿½ç•¥å·²å¤„ç†çš„æ ‡è®°ï¼‰ï¼Œé»˜è®¤False
    """
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    # ä»GDLæ–‡ä»¶åŠ è½½åˆæ³•çš„è°“è¯
    valid_predicates = load_valid_predicates(gdl_path)
    if not valid_predicates:
        print("ç”±äºæ— æ³•åŠ è½½è°“è¯åº“ï¼Œå¤„ç†ä¸­æ­¢ã€‚")
        return
    
    # å°†è°“è¯åˆ—è¡¨æ ¼å¼åŒ–ä¸ºå­—ç¬¦ä¸²
    valid_predicates_str = ", ".join(valid_predicates)

    # ä»manual_train_setåŠ è½½few-shotèŒƒä¾‹ï¼ˆåŒ…å«å›¾ç‰‡ï¼‰
    train_dir = example_dir  # example_dirç°åœ¨æŒ‡å‘manual_train_set
    images_dir = "src/fgps/formalgeo7k_v2/images"  # å›¾ç‰‡ç›®å½•
    # ä½¿ç”¨é€‚é‡çš„few-shotæ ·æœ¬ï¼ˆ20ä¸ªï¼Œåœ¨æ•ˆæœå’Œæ€§èƒ½ä¹‹é—´å–å¾—å¹³è¡¡ï¼‰
    # 20ä¸ªèŒƒä¾‹åŒ…å«å›¾ç‰‡ï¼Œè¯·æ±‚å¤§å°é€‚ä¸­ï¼Œæ—¢èƒ½æä¾›è¶³å¤Ÿçš„å­¦ä¹ æ ·æœ¬åˆä¸ä¼šå¯¼è‡´è¶…æ—¶
    few_shot_examples_data, example_count = load_few_shot_examples(train_dir, images_dir, max_examples=20)
    
    if example_count == 0:
        print("è­¦å‘Š: æœªèƒ½åŠ è½½ few-shot èŒƒä¾‹ï¼Œå°†ç»§ç»­ä½¿ç”¨æ— èŒƒä¾‹çš„æç¤ºè¯ã€‚")
    
    # æ„å»ºèŒƒä¾‹æ–‡æœ¬ï¼ˆç”¨äºæç¤ºè¯ï¼‰
    examples_text = ""
    for i, example in enumerate(few_shot_examples_data, 1):
        img_note = f"å›¾ç‰‡è·¯å¾„ï¼š{example['problem_id']}.png" if example['image_base64'] else "æ— å¯¹åº”å›¾ç‰‡"
        examples_text += f"""
#### èŒƒä¾‹{i}ï¼ˆIDï¼š{example['problem_id']}ï¼‰
- é—®é¢˜æ–‡æœ¬ï¼š{example['problem_text']}
- {img_note}
- text_cdlï¼š{example['text_cdl']}
- image_cdlï¼š{example['image_cdl']}
- construction_cdlï¼š{example['construction_cdl']}
- goal_cdlï¼š{example['goal_cdl']}
- problem_answerï¼š{example['problem_answer']}
"""
    
    # åŠ¨æ€æ„å»ºé»„é‡‘æç¤ºè¯ï¼ˆå‚è€ƒGeminiä»£ç é£æ ¼ï¼‰
    golden_prompt = PROMPT_TEMPLATE.format(
        valid_predicates_str=valid_predicates_str,
        total_examples=example_count,
        training_examples=examples_text
    )
    
    # æŸ¥æ‰¾è¾“å…¥æ–‡ä»¶
    problem_files = [f for f in os.listdir(input_dir) if f.endswith('.json')]
    
    # è¿‡æ»¤é—®é¢˜IDèŒƒå›´
    filtered_files = []
    for json_filename in problem_files:
        try:
            problem_id = int(json_filename.split('.')[0])
            if problem_id >= start_id and (end_id is None or problem_id <= end_id):
                filtered_files.append(json_filename)
        except ValueError:
            # å¦‚æœæ— æ³•è§£æIDï¼Œè·³è¿‡è¯¥æ–‡ä»¶
            continue
    
    filtered_files.sort(key=lambda x: int(x.split('.')[0]))
    
    # æ–­ç‚¹ç»­ä¼ ï¼šåŠ è½½å·²æœ‰çš„æ—¥å¿—æ–‡ä»¶ï¼Œæ£€æŸ¥å·²å¤„ç†çš„é—®é¢˜
    log_file_path = os.path.join(output_dir, "_generation_log_chatgpt.json")
    processed_problems = set()  # å·²æˆåŠŸå¤„ç†çš„é—®é¢˜IDé›†åˆ
    log_data = []  # æ—¥å¿—æ•°æ®
    
    # å¦‚æœå¼ºåˆ¶é‡æ–°ç”Ÿæˆï¼Œè·³è¿‡åŠ è½½å·²å¤„ç†çš„é—®é¢˜
    if force_regenerate:
        print("ğŸ”„ Force regenerate mode: Will regenerate all problems regardless of existing outputs.")
        # å¯ä»¥é€‰æ‹©å¤‡ä»½æ—§çš„æ—¥å¿—æ–‡ä»¶
        if os.path.exists(log_file_path):
            backup_path = log_file_path + ".backup"
            try:
                import shutil
                shutil.copy2(log_file_path, backup_path)
                print(f"ğŸ“¦ Backed up existing log to: {backup_path}")
            except Exception as e:
                print(f"âš ï¸  Warning: Failed to backup log file: {e}")
    else:
        # æ­£å¸¸æ¨¡å¼ï¼šåŠ è½½å·²å¤„ç†çš„é—®é¢˜
        if os.path.exists(log_file_path):
            try:
                with open(log_file_path, 'r', encoding='utf-8') as f:
                    existing_logs = json.load(f)
                # æ‰¾å‡ºå·²æˆåŠŸå¤„ç†çš„é—®é¢˜
                for log_entry in existing_logs:
                    if log_entry.get("status") == "success":
                        processed_problems.add(str(log_entry.get("problem_id")))
                log_data = existing_logs  # ä¿ç•™å·²æœ‰æ—¥å¿—
                print(f"ğŸ“‹ Loaded existing log: Found {len(processed_problems)} successfully processed problems, will continue from checkpoint...")
            except Exception as e:
                print(f"âš ï¸  Warning: Unable to load existing log file '{log_file_path}': {e}, will start from beginning.")
        
        # æ£€æŸ¥è¾“å‡ºç›®å½•ä¸­å·²å­˜åœ¨çš„æ–‡ä»¶ï¼ˆå³ä½¿æ—¥å¿—ä¸­æ²¡æœ‰è®°å½•ï¼‰
        if os.path.exists(output_dir):
            existing_output_files = [f for f in os.listdir(output_dir) if f.endswith('.json') and f != '_generation_log_chatgpt.json']
            for output_file in existing_output_files:
                problem_id_from_file = output_file.split('.')[0]
                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æœ‰æ•ˆï¼ˆåŒ…å«å¿…è¦å­—æ®µï¼‰
                output_file_path = os.path.join(output_dir, output_file)
                try:
                    with open(output_file_path, 'r', encoding='utf-8') as f:
                        output_data = json.load(f)
                    # å¦‚æœæ–‡ä»¶åŒ…å«problem_idå­—æ®µï¼Œè®¤ä¸ºå·²å¤„ç†ï¼ˆéœ€è¦æ£€æŸ¥æ˜¯å¦æ˜¯dictï¼‰
                    if isinstance(output_data, dict) and output_data.get("problem_id") is not None:
                        processed_problems.add(problem_id_from_file)
                except (json.JSONDecodeError, IOError, OSError):
                    pass  # å¦‚æœæ–‡ä»¶æŸåæˆ–æ— æ³•è¯»å–ï¼Œå¿½ç•¥
    
    # è¿‡æ»¤æ‰å·²å¤„ç†çš„é—®é¢˜
    remaining_files = []
    for json_filename in filtered_files:
        problem_id_str = json_filename.split('.')[0]
        if problem_id_str not in processed_problems:
            remaining_files.append(json_filename)
    
    print(f"Found {len(problem_files)} problem files, filtered to {len(filtered_files)} problems in range, of which {len(processed_problems)} are already processed, {len(remaining_files)} remaining to process.")
    
    if len(remaining_files) == 0:
        print("âœ… All problems have been processed!")
        return
    
    for idx, json_filename in enumerate(tqdm(remaining_files, desc="ChatGPT Processing Progress"), 1):
        problem_id_str = json_filename.split('.')[0]
        json_path = os.path.join(input_dir, json_filename)
        log_entry = {"problem_id": problem_id_str}

        try:
            # ä»JSONä¸­è¯»å– problem_text å’Œ image_path
            with open(json_path, 'r', encoding='utf-8') as f:
                input_data = json.load(f)
            
            # è·å–é—®é¢˜æ–‡æœ¬ï¼ˆå…è®¸ä¸ºç©ºï¼Œåªè¦æœ‰å›¾ç‰‡å°±å¯ä»¥å¤„ç†ï¼‰
            problem_text = input_data.get("problem_text_en", "").strip()
            if not problem_text:
                problem_text = input_data.get("problem_text_cn", "").strip()
            if not problem_text:
                problem_text = ""  # å…è®¸ä¸ºç©ºï¼Œåªå¤„ç†å›¾ç‰‡
            
            # ä»æ–‡ä»¶åæˆ–problem_idå­—æ®µä¸­æå–é—®é¢˜ID
            problem_id = input_data.get("problem_id")
            if problem_id is None:
                # å¦‚æœJSONä¸­æ²¡æœ‰problem_idï¼Œä»æ–‡ä»¶åä¸­æå–
                try:
                    problem_id = int(problem_id_str)
                except ValueError:
                    print(f"Warning: Cannot extract problem_id from filename '{json_filename}', skipping.")
                    log_entry.update({"status": "skipped", "reason": "Cannot extract problem_id from filename"})
                    log_data.append(log_entry)
                    continue
            else:
                problem_id = int(problem_id)
            
            # ä¼˜å…ˆä½¿ç”¨ problem_img å­—æ®µä¸­çš„è·¯å¾„
            image_path = None
            images_base_dir = "src/fgps/formalgeo7k_v2/images"
            image_extensions = ['.png', '.jpg', '.jpeg']
            
            # æ–¹æ³•1: å°è¯•ä» problem_img å­—æ®µè·å–å›¾ç‰‡è·¯å¾„
            problem_img = input_data.get("problem_img")
            if problem_img:
                if isinstance(problem_img, list) and len(problem_img) > 0:
                    img_path_str = problem_img[0]
                elif isinstance(problem_img, str):
                    img_path_str = problem_img
                else:
                    img_path_str = None
                
                if img_path_str:
                    # å¤„ç†è·¯å¾„æ ¼å¼ï¼ˆå¯èƒ½æ˜¯ Windows è·¯å¾„æ ¼å¼ images\239.jpgï¼‰
                    img_path_str = img_path_str.replace('\\', '/')
                    # æå–æ–‡ä»¶å
                    img_filename = os.path.basename(img_path_str)
                    # å°è¯•åœ¨ images ç›®å½•ä¸­æŸ¥æ‰¾
                    candidate_path = os.path.join(images_base_dir, img_filename)
                    if os.path.exists(candidate_path):
                        image_path = candidate_path
            
            # æ–¹æ³•2: å¦‚æœæ–¹æ³•1å¤±è´¥ï¼ŒæŒ‰ problem_id æŸ¥æ‰¾å›¾ç‰‡æ–‡ä»¶
            if not image_path:
                for ext in image_extensions:
                    candidate_path = os.path.join(images_base_dir, f"{problem_id}{ext}")
                    if os.path.exists(candidate_path):
                        image_path = candidate_path
                        break
            
            # å¦‚æœä»ç„¶æ‰¾ä¸åˆ°å›¾ç‰‡ï¼Œæ£€æŸ¥æ˜¯å¦è‡³å°‘æœ‰é—®é¢˜æ–‡æœ¬
            if not image_path:
                if not problem_text:
                    print(f"Warning: Problem {problem_id} has no image and no text, skipping.")
                    log_entry.update({"status": "skipped", "reason": f"No image and no text for problem_id {problem_id}"})
                    log_data.append(log_entry)
                    continue
                else:
                    # åªæœ‰æ–‡æœ¬æ²¡æœ‰å›¾ç‰‡ï¼Œä»ç„¶å¯ä»¥å¤„ç†ï¼ˆä½†ä¼šæç¤ºï¼‰
                    print(f"Warning: Problem {problem_id} has no image file, will process with text only.")
                    image_path = None  # è®¾ç½®ä¸º Noneï¼Œåç»­ä¼šå¤„ç†

        except (json.JSONDecodeError, KeyError) as e:
            print(f"è­¦å‘Š: è§£æJSONæ–‡ä»¶ '{json_filename}' æ—¶å‡ºé”™: {e}ï¼Œå·²è·³è¿‡ã€‚")
            log_entry.update({"status": "skipped", "reason": f"Error reading source JSON: {e}"})
            log_data.append(log_entry)
            continue
        
        # æ˜¾ç¤ºå½“å‰å¤„ç†è¿›åº¦
        if idx % 10 == 1 or idx == len(remaining_files):
            print(f"\n[{idx}/{len(remaining_files)}] Processing problem {problem_id_str}...")
        
        # è°ƒç”¨APIç”Ÿæˆï¼ˆä¼ å…¥few-shotèŒƒä¾‹æ•°æ®ï¼‰
        print(f"  ğŸ“¤ Calling API for problem {problem_id_str}...")
        # å¦‚æœ image_path ä¸º Noneï¼Œä¼ é€’ None è€Œä¸æ˜¯å­—ç¬¦ä¸²
        result = generate_geometry_json(problem_text, image_path if image_path else None, golden_prompt, problem_id, few_shot_examples_data)
        
        # æ˜¾ç¤ºå¤„ç†ç»“æœ
        if result["status"] == "success":
            print(f"  âœ… Problem {problem_id_str} processed successfully")
        else:
            print(f"  âŒ Problem {problem_id_str} failed: {result.get('message', 'Unknown error')[:100]}")
        
        # ä¿å­˜å’Œæ—¥å¿—è®°å½•
        if result["status"] == "success":
            output_json_path = os.path.join(output_dir, f"{problem_id_str}.json")
            with open(output_json_path, 'w', encoding='utf-8') as f:
                json.dump(result["data"], f, indent=2, ensure_ascii=False)
            log_entry["status"] = "success"
            log_entry["output_file"] = output_json_path
            processed_problems.add(problem_id_str)  # æ ‡è®°ä¸ºå·²å¤„ç†
        else:
            log_entry["status"] = "error"
            log_entry["reason"] = result["message"]
            if "raw_output" in result:
                log_entry["raw_output"] = result["raw_output"]
        
        # æ›´æ–°æ—¥å¿—ï¼ˆæŸ¥æ‰¾æ˜¯å¦å·²æœ‰è¯¥é—®é¢˜çš„æ—¥å¿—æ¡ç›®ï¼‰
        existing_entry_index = None
        for i, entry in enumerate(log_data):
            if entry.get("problem_id") == problem_id_str:
                existing_entry_index = i
                break
        
        if existing_entry_index is not None:
            # æ›´æ–°å·²æœ‰æ¡ç›®
            log_data[existing_entry_index] = log_entry
        else:
            # æ·»åŠ æ–°æ¡ç›®
            log_data.append(log_entry)
        
        # å¢é‡ä¿å­˜æ—¥å¿—ï¼ˆæ¯å¤„ç†ä¸€ä¸ªé—®é¢˜å°±ä¿å­˜ä¸€æ¬¡ï¼Œå®ç°æ–­ç‚¹ç»­ä¼ ï¼‰
        try:
            with open(log_file_path, 'w', encoding='utf-8') as f:
                json.dump(log_data, f, indent=2, ensure_ascii=False)
        except Exception as e:
            print(f"âš ï¸  Warning: Failed to save log file: {e}")

    # æœ€ç»ˆä¿å­˜æ—¥å¿—ï¼ˆç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½å·²ä¿å­˜ï¼‰
    with open(log_file_path, 'w', encoding='utf-8') as f:
        json.dump(log_data, f, indent=2, ensure_ascii=False)

    print(f"\næ‰¹é‡å¤„ç†å®Œæˆï¼ç»“æœå·²ä¿å­˜åœ¨ '{output_dir}' ç›®å½•ã€‚")
    print(f"è¯¦ç»†æ—¥å¿—è¯·è§: '{log_file_path}'")


if __name__ == '__main__':
    # --- è¯·åœ¨è¿™é‡Œé…ç½®æ‚¨çš„æ–‡ä»¶å¤¹è·¯å¾„ ---
    # å­˜æ”¾æ–°é—®é¢˜ï¼ˆ.jsonæ–‡ä»¶ï¼‰çš„æ–‡ä»¶å¤¹ - ç›´æ¥ä»åŸå§‹é—®é¢˜ç›®å½•è¯»å–
    INPUT_PROBLEM_DIR = "src/fgps/formalgeo7k_v2/problems" 
    # å­˜æ”¾æ‰‹åŠ¨è®­ç»ƒé›†ï¼ˆmanual_train_setï¼‰çš„æ–‡ä»¶å¤¹
    FEW_SHOT_EXAMPLE_DIR = "gemini/data/manual_train_set"
    # å­˜æ”¾AIç”Ÿæˆç»“æœçš„æ–‡ä»¶å¤¹
    OUTPUT_DIR = "gemini/data/chatgpt_output"
    # è°“è¯å®šä¹‰æ–‡ä»¶
    PREDICATE_GDL_PATH = "gemini/predicate_GDL.json"
    # å¤„ç†èŒƒå›´ï¼š1-700é¢˜ç›®
    START_ID = 1
    END_ID = 700

    if not os.path.exists(INPUT_PROBLEM_DIR):
        print(f"Error: Input directory does not exist: {INPUT_PROBLEM_DIR}")
        exit(1)
    if not os.path.exists(FEW_SHOT_EXAMPLE_DIR):
        print(f"Warning: Example directory does not exist: {FEW_SHOT_EXAMPLE_DIR}, will continue without examples.")
    
    # æ˜¯å¦å¼ºåˆ¶é‡æ–°ç”Ÿæˆæ‰€æœ‰é—®é¢˜ï¼ˆå¿½ç•¥å·²å¤„ç†çš„æ ‡è®°ï¼‰
    # è®¾ç½®ä¸º True å°†é‡æ–°ç”Ÿæˆæ‰€æœ‰é—®é¢˜ï¼Œå³ä½¿ä¹‹å‰å·²ç»å¤„ç†è¿‡
    FORCE_REGENERATE = True  # è®¾ç½®ä¸º True ä»¥é‡æ–°ç”Ÿæˆæ‰€æœ‰é—®é¢˜
    
    # è¿è¡Œæ‰¹é‡å¤„ç†
    batch_process_problems(
        input_dir=INPUT_PROBLEM_DIR, 
        output_dir=OUTPUT_DIR, 
        example_dir=FEW_SHOT_EXAMPLE_DIR,
        gdl_path=PREDICATE_GDL_PATH,
        start_id=START_ID,
        end_id=END_ID,
        force_regenerate=FORCE_REGENERATE
    )

